"""
tour_api_loader.py (ê°œì„  ë²„ì „)
==============================
í•œêµ­ê´€ê´‘ê³µì‚¬_êµ­ë¬¸ ê´€ê´‘ì •ë³´ ì„œë¹„ìŠ¤(GW) â†’ contentTypeIdë³„ ë¶„ë¦¬ ìˆ˜ì§‘

â¶ .env ì— ë°˜ë“œì‹œ ë‘ ë³€ìˆ˜ë¥¼ ë„£ì–´ ì£¼ì„¸ìš”
   TOUR_BASE_URL=https://apis.data.go.kr/B551011/KorService2
   TOUR_API_KEY=ë°œê¸‰ë°›ì€í‚¤
â· ì‹¤í–‰ ë°©ë²•:
   python -m scripts.tour_api_loader        # contentTypeë³„ ë¶„ë¦¬ ìˆ˜ì§‘
   python -m scripts.init_db                # ì„ë² ë”© ì¬ê³„ì‚°

ì£¼ìš” ê°œì„ ì‚¬í•­:
- contentTypeIdë³„ ê°œë³„ CSV íŒŒì¼ ìƒì„±
- ê° íŒŒì¼ì´ ì–´ë–¤ ë°ì´í„°ì¸ì§€ ì§ê´€ì  íŒŒì¼ëª…
- ê°œë°œ ê³¼ì •ì—ì„œ ë°ì´í„° ì¶”ì  ìš©ì´
"""

from __future__ import annotations
import httpx, pandas as pd, time
from pathlib import Path
from typing import List, Dict

from sqlalchemy.orm import Session

from app.config import get_settings
from app.db.database import SessionLocal, Base, engine
from app.db import models
from app.embeddings.embedding_service import embed_texts
from app.utils.keyword_search import get_keyword_service
import json

# httpx í´ë¼ì´ì–¸íŠ¸: ì—°ê²°Â·ì „ì²´ ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì„¤ì •
CLIENT = httpx.Client(timeout=httpx.Timeout(30.0, connect=10.0))

# í™˜ê²½ì„¤ì • ë¡œë“œ (.env â†’ Pydantic Settings)
settings = get_settings()

# contentTypeIdë³„ íŒŒì¼ëª… ë§¤í•‘
CONTENT_TYPE_FILES = {
    12: "tour_api_attractions.csv",      # ê´€ê´‘ì§€ (ê¸°ì¡´ tour_api_with_keywords.csvì™€ ë™ì¼)
    14: "tour_api_cultural.csv",         # ë¬¸í™”ì‹œì„¤
    15: "tour_api_festivals.csv",        # ì¶•ì œ/ê³µì—°/í–‰ì‚¬  
    25: "tour_api_courses.csv",          # ì—¬í–‰ì½”ìŠ¤
    28: "tour_api_leisure.csv",          # ë ˆí¬ì¸ 
    38: "tour_api_shopping.csv",         # ì‡¼í•‘
    32: "tour_api_accommodations.csv",   # ìˆ™ë°• (ë³„ë„ ìŠ¤í¬ë¦½íŠ¸)
    39: "tour_api_restaurants.csv"       # ìŒì‹ì  (ë³„ë„ ìŠ¤í¬ë¦½íŠ¸)
}

BASE_URL: str = settings.tour_base_url.rstrip("/")          # KorService2 ë² ì´ìŠ¤ URL
SERVICE_KEY: str = settings.tour_api_key                     # ê°œì¸ ì¸ì¦í‚¤

print(f"ğŸ”§ í™˜ê²½ ì„¤ì • í™•ì¸:")
print(f"   BASE_URL: {BASE_URL}")
print(f"   SERVICE_KEY: {'*' * 10}{SERVICE_KEY[-10:] if len(SERVICE_KEY) > 10 else SERVICE_KEY}")
print()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API í˜¸ì¶œ & í˜ì´ì§€ë„¤ì´ì…˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# API ê³µí†µ íŒŒë¼ë¯¸í„° (listYN ì œê±°)
DEFAULT_PARAMS = {
    "serviceKey": SERVICE_KEY,
    "MobileOS": "ETC",
    "MobileApp": "KDT-AgricultureTourApp",
    "_type": "json",
    "arrange": "A",     # ì œëª© ìˆœ ì •ë ¬  
    "numOfRows": 100,   # í˜ì´ì§€ë‹¹ 100ê°œ
    "pageNo": 1
}

def fetch_area_list(
    page: int = 1,
    contentTypeId=None,        # Noneì´ë©´ ëª¨ë“  ë¶„ë¥˜ ìˆ˜ì§‘
    areaCode=None,
    sigunguCode=None,
    max_retries: int = 3
) -> tuple[list[dict], int]:
    """areaBasedList2 API í˜¸ì¶œ (í˜ì´ì§€ë³„)."""
    
    params = DEFAULT_PARAMS.copy()
    params["pageNo"] = page
    
    if areaCode:
        params["areaCode"] = areaCode
    if sigunguCode:
        params["sigunguCode"] = sigunguCode
    if contentTypeId:
        params["contentTypeId"] = contentTypeId

    url = f"{BASE_URL}/areaBasedList2"
    
    for attempt in range(max_retries):
        try:
            print(f"   ğŸ“¡ API í˜¸ì¶œ: í˜ì´ì§€ {page} (ì‹œë„ {attempt + 1}/{max_retries})")
            response = CLIENT.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            # ì‘ë‹µ ë‚´ìš© ë””ë²„ê¹…
            response_text = response.text.strip()
            if not response_text:
                print(f"   âš ï¸ ë¹ˆ ì‘ë‹µ ë°›ìŒ (í˜ì´ì§€ {page})")
                return [], 0
                
            if not response_text.startswith('{'):
                print(f"   âš ï¸ JSONì´ ì•„ë‹Œ ì‘ë‹µ ë°›ìŒ (í˜ì´ì§€ {page}): {response_text[:200]}...")
                return [], 0
            
            data = response.json()
            
            # ë””ë²„ê¹…: API ì‘ë‹µ êµ¬ì¡° ì¶œë ¥
            print(f"   ğŸ” API ì‘ë‹µ êµ¬ì¡°: {json.dumps(data, ensure_ascii=False, indent=2)[:500]}...")
            
            # TourAPI ì‘ë‹µ êµ¬ì¡° íŒŒì‹±
            if "response" not in data:
                print(f"   âŒ ì‘ë‹µì— 'response' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {data}")
                return [], 0
                
            response_data = data["response"]
            print(f"   ğŸ“‹ response ë°ì´í„°: {response_data}")
            
            header = response_data.get("header", {})
            print(f"   ğŸ“„ header: {header}")
            
            body = response_data.get("body", {})
            if not body or body.get("totalCount", 0) == 0:
                print(f"   âš ï¸ bodyê°€ ì—†ê±°ë‚˜ totalCountê°€ 0ì…ë‹ˆë‹¤: {body}")
                return [], 0
            
            items = body.get("items", {})
            if not items:
                return [], body.get("totalCount", 0)
                
            item_list = items.get("item", [])
            if not isinstance(item_list, list):
                item_list = [item_list]  # ë‹¨ì¼ ì•„ì´í…œì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                
            total_count = body.get("totalCount", len(item_list))
            return item_list, total_count
            
        except Exception as e:
            print(f"   âŒ API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
            else:
                raise e

    return [], 0

def generate_tags_by_content_type(content_type_id, cat1):
    """contentTypeIdì™€ cat1ì— ë”°ë¼ ì ì ˆí•œ íƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    tags = []
    
    # contentTypeIdë³„ íƒœê·¸ ë§¤í•‘
    type_mapping = {
        12: "ê´€ê´‘ì§€",
        14: "ë¬¸í™”ì‹œì„¤", 
        15: "ì¶•ì œ",
        25: "ì—¬í–‰ì½”ìŠ¤",
        28: "ë ˆí¬ì¸ ",
        38: "ì‡¼í•‘",
        32: "ìˆ™ë°•",
        39: "ìŒì‹ì "
    }
    
    if content_type_id and int(content_type_id) in type_mapping:
        tags.append(type_mapping[int(content_type_id)])
    
    # cat1 ë¶„ë¥˜ë³„ ì„¸ë¶€ íƒœê·¸
    if cat1:
        cat1_mapping = {
            "A01": "ìì—°",
            "A02": "ì¸ë¬¸",
            "A03": "ë ˆí¬ì¸ ", 
            "A04": "ì‡¼í•‘",
            "A05": "ìŒì‹",
            "B02": "ìˆ™ë°•",
            "C01": "ì¶”ì²œì½”ìŠ¤"
        }
        if cat1 in cat1_mapping:
            tags.append(cat1_mapping[cat1])
    
    return ",".join(tags) if tags else "ê¸°íƒ€"

def to_dataframe(tour_items: list[dict]) -> pd.DataFrame:
    """TourAPI ì‘ë‹µì„ DataFrameìœ¼ë¡œ ë³€í™˜."""
    rows = []
    
    for it in tour_items:
        # ì¢Œí‘œ ë³€í™˜ (ë¬¸ìì—´ â†’ float)
        try:
            longitude = float(it.get("mapx", 0)) if it.get("mapx") else None
            latitude = float(it.get("mapy", 0)) if it.get("mapy") else None
        except (ValueError, TypeError):
            longitude = None
            latitude = None
        
        rows.append(
            dict(
                # TourSpot ëª¨ë¸ í•„ë“œì™€ ì¼ì¹˜
                id=None,                                         # ìë™ ì¦ê°€
                name=it.get("title", "ì œëª©ì—†ìŒ"),
                region=it.get("addr1", "ì£¼ì†Œì—†ìŒ"),
                lat=latitude,                                   # latitude â†’ lat
                lon=longitude,                                  # longitude â†’ lon
                contentid=it.get("contentid", ""),           # TourAPI contentid
                # contentTypeIdì— ë”°ë¥¸ íƒœê·¸ ìƒì„±
                tags=generate_tags_by_content_type(it.get("contenttypeid"), it.get("cat1")),
                image_url=None,                                 # ê¸°ë³¸ê°’
                detailed_keywords="[]",                         # ê¸°ë³¸ê°’
                keywords=None                                   # ê¸°ë³¸ê°’
            )
        )
    
    return pd.DataFrame(rows)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ë¯¸ì§€ ìˆ˜ì§‘ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_detail_image(contentid: str) -> str | None:
    """TourAPI detailImage2 ì—”ë“œí¬ì¸íŠ¸ë¡œ ê´€ê´‘ì§€ ì´ë¯¸ì§€ URLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Parameters
    ----------
    contentid : str
        TourAPI contentid
        
    Returns
    -------
    str | None
        ëŒ€í‘œ ì´ë¯¸ì§€ URL ë˜ëŠ” None (ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°)
    """
    if not contentid:
        return None
        
    params = {
        "serviceKey": SERVICE_KEY,
        "MobileOS": "ETC",
        "MobileApp": "KDT-AgricultureTourApp",
        "contentId": contentid,
        "imageYN": "Y",
        "numOfRows": 1,  # ëŒ€í‘œ ì´ë¯¸ì§€ 1ê°œë§Œ
        "_type": "json"
    }
    
    url = f"{BASE_URL}/detailImage2"
    
    try:
        r = CLIENT.get(url, params=params)
        r.raise_for_status()
        body = r.json()["response"]["body"]
        
        items_field = body.get("items")
        if not items_field:
            return None
            
        if isinstance(items_field, dict):
            raw_items = items_field.get("item", [])
            items = raw_items if isinstance(raw_items, list) else [raw_items]
        elif isinstance(items_field, list):
            items = items_field
        else:
            return None
            
        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ originimgurl ë°˜í™˜
        if items and len(items) > 0:
            return items[0].get("originimgurl")
            
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨ (contentid: {contentid}): {e}")
        
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_existing_data_for_type(content_type_id: int) -> tuple[pd.DataFrame | None, set]:
    """íŠ¹ì • contentTypeIdì˜ ê¸°ì¡´ CSV íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    file_path = Path("data") / CONTENT_TYPE_FILES[content_type_id]
    
    if file_path.exists():
        try:
            existing_df = pd.read_csv(file_path)
            existing_contentids = set(existing_df["contentid"].astype(str))
            print(f"ğŸ“„ ê¸°ì¡´ {CONTENT_TYPE_FILES[content_type_id]} íŒŒì¼ ë¡œë“œ: {len(existing_df)}ê±´")
            return existing_df, existing_contentids
        except Exception as e:
            print(f"âŒ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({CONTENT_TYPE_FILES[content_type_id]}): {e}")
            return None, set()
    else:
        print(f"ğŸ“„ {CONTENT_TYPE_FILES[content_type_id]} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        return None, set()

def collect_all_content_type_data(content_type_id: int, type_name: str, existing_contentids: set) -> list[dict]:
    """íŠ¹ì • contentTypeIdì˜ ë°ì´í„°ë¥¼ ì „ë¶€ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (ê°œìˆ˜ ì œí•œ ì—†ìŒ, ì¤‘ë³µ ì œì™¸)."""
    print(f"ğŸ” {type_name} ë°ì´í„° ì „ì²´ ìˆ˜ì§‘ ì‹œì‘ (contentTypeId: {content_type_id})...")
    
    all_items: list[dict] = []
    new_items_count = 0
    
    # ì²« í˜ì´ì§€ë¡œ ì „ì²´ ê°œìˆ˜ íŒŒì•…
    items, total_count = fetch_area_list(1, content_type_id)
    
    if not items and total_count == 0:
        print(f"âŒ {type_name} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
        
    # ì¤‘ë³µ ì²´í¬í•˜ì—¬ ì‹ ê·œ ë°ì´í„°ë§Œ ì¶”ê°€
    for item in items:
        if item.get("contentid", "") not in existing_contentids:
            all_items.append(item)
            new_items_count += 1
    
    # ì „ì²´ í˜ì´ì§€ ìˆ˜ ê³„ì‚°
    page_size = DEFAULT_PARAMS["numOfRows"]
    total_pages = (total_count + page_size - 1) // page_size
    
    print(f"   {type_name}: ì „ì²´ {total_count}ê°œ ë°ì´í„°, {total_pages}í˜ì´ì§€ (ì „ì²´ ìˆ˜ì§‘)")
    print(f"   ğŸ”„ 1/{total_pages} í˜ì´ì§€ ì™„ë£Œ - ì‹ ê·œ: {new_items_count}ê±´")
    
    # ë‚˜ë¨¸ì§€ í˜ì´ì§€ ìˆ˜ì§‘
    for page in range(2, total_pages + 1):
        try:
            items, _ = fetch_area_list(page, content_type_id)
            if not items:
                print(f"   âš ï¸  {page}í˜ì´ì§€ì—ì„œ ë°ì´í„°ê°€ ì—†ì–´ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
        except Exception as e:
            print(f"   âŒ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {e}")
            continue  # ì‹¤íŒ¨í•œ í˜ì´ì§€ëŠ” ê±´ë„ˆë›°ê³  ê³„ì†
        
        # ì¤‘ë³µ ì²´í¬
        page_new_count = 0
        for item in items:
            if item.get("contentid", "") not in existing_contentids:
                all_items.append(item)
                page_new_count += 1
                
        new_items_count += page_new_count
        
        if page % 10 == 0:  # 10í˜ì´ì§€ë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
            print(f"   ğŸ”„ {page}/{total_pages} í˜ì´ì§€ ì™„ë£Œ - ì´ ì‹ ê·œ ëˆ„ì : {new_items_count}ê±´")
        time.sleep(0.2)
    
    print(f"âœ… {type_name} ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_items)}ê±´ ì‹ ê·œ ë°ì´í„°")
    return all_items

def save_type_specific_data(content_type_id: int, new_df: pd.DataFrame, existing_df: pd.DataFrame | None):
    """contentTypeë³„ë¡œ ê°œë³„ CSV íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    filename = CONTENT_TYPE_FILES[content_type_id]
    file_path = Path("data") / filename
    
    # ì´ë¯¸ì§€ URLê³¼ detailed_keywords ê¸°ë³¸ê°’ ì„¤ì •
    if "image_url" not in new_df.columns:
        new_df["image_url"] = None
    if "detailed_keywords" not in new_df.columns:
        new_df["detailed_keywords"] = "[]"
    
    # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
    if existing_df is not None:
        # ê¸°ì¡´ ë°ì´í„°ì— ì»¬ëŸ¼ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬
        if "image_url" not in existing_df.columns:
            existing_df["image_url"] = None
        if "detailed_keywords" not in existing_df.columns:
            existing_df["detailed_keywords"] = "[]"
            
        final_df = pd.concat([existing_df, new_df], ignore_index=True)
    else:
        final_df = new_df
    
    # CSV íŒŒì¼ ì €ì¥
    Path("data").mkdir(exist_ok=True)
    final_df.to_csv(file_path, index=False)
    print(f"âœ… {filename} ì €ì¥ ì™„ë£Œ: {len(final_df)}ê±´")
    
    return final_df

def main():
    print("ğŸŒŸ contentTypeë³„ ë¶„ë¦¬ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("ğŸ“Œ TOUR_DATA_SYSTEM_GUIDE.mdì— ë”°ë¼ 6ê°œ ê´€ê´‘ ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤")
    print("   (ê´€ê´‘ì§€, ë¬¸í™”ì‹œì„¤, ì¶•ì œ, ì—¬í–‰ì½”ìŠ¤, ë ˆí¬ì¸ , ì‡¼í•‘)\n")
    print("   ìˆ™ë°•/ìŒì‹ì ì€ ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ë¡œ ìˆ˜ì§‘í•˜ì„¸ìš”: accommodation_restaurant_loader.py\n")

    # 1) í…Œì´ë¸” ìƒì„±
    Base.metadata.create_all(bind=engine)
    db: Session = SessionLocal()

    # 2) ìˆ˜ì§‘í•  contentType ì •ì˜ (ê°€ì´ë“œ ë¬¸ì„œ ê¸°ì¤€ ì „ì²´ ìˆ˜ì§‘)
    content_types = [
        (12, "ê´€ê´‘ì§€"),
        (14, "ë¬¸í™”ì‹œì„¤"), 
        (15, "ì¶•ì œ/ê³µì—°/í–‰ì‚¬"),
        (25, "ì—¬í–‰ì½”ìŠ¤"),
        (28, "ë ˆí¬ì¸ "),
        (38, "ì‡¼í•‘")
    ]
    
    print("ğŸŒŸ ì „ì²´ ëª¨ë“œ: ê°€ì´ë“œ ë¬¸ì„œì— ë”°ë¼ 6ê°œ contentTypeì„ ëª¨ë‘ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    
    total_new_items = 0
    all_db_items = []  # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ìš©
    
    # 3) contentTypeë³„ë¡œ ê°œë³„ ì²˜ë¦¬
    for content_type_id, type_name in content_types:
        print(f"\n{'='*60}")
        print(f"ğŸ”„ {type_name} (contentTypeId: {content_type_id}) ì²˜ë¦¬ ì‹œì‘")
        print(f"{'='*60}")
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        existing_df, existing_contentids = load_existing_data_for_type(content_type_id)
        
        # ì‹ ê·œ ë°ì´í„° ìˆ˜ì§‘
        new_items = collect_all_content_type_data(content_type_id, type_name, existing_contentids)
        
        if new_items:
            print(f"ğŸ’¾ {type_name} ì‹ ê·œ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {len(new_items)}ê±´")
            new_df = to_dataframe(new_items)
            
            # contentTypeë³„ íŒŒì¼ ì €ì¥
            final_df = save_type_specific_data(content_type_id, new_df, existing_df)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ìš© ëˆ„ì 
            all_db_items.extend(new_df.to_dict("records"))
            total_new_items += len(new_items)
            
            print(f"âœ… {type_name} ì²˜ë¦¬ ì™„ë£Œ (ì‹ ê·œ {len(new_items)}ê±´, ì´ {len(final_df)}ê±´)")
        else:
            print(f"ğŸ‰ {type_name}: ì‹ ê·œ ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        time.sleep(2.0)  # API ì•ˆì •ì„±ì„ ìœ„í•œ ê°„ê²©

    # 4) ë°ì´í„°ë² ì´ìŠ¤ ì¼ê´„ ì €ì¥ (ì‹ ê·œ ë°ì´í„°ë§Œ)
    if all_db_items:
        print(f"\nğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ì— ì‹ ê·œ ë°ì´í„° ì¼ê´„ ì €ì¥ ì¤‘: {len(all_db_items)}ê±´...")
        spots = [models.TourSpot(**row) for row in all_db_items]
        db.bulk_save_objects(spots, return_defaults=False)
        db.commit()
        print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")

        # 5) ì„ë² ë”© ë²¡í„° ìƒì„± (ì‹ ê·œ ë°ì´í„°ë§Œ)
        print("ğŸ§  OpenAI ì„ë² ë”© ë²¡í„° ìƒì„± ì¤‘...")
        tag_texts = [row["tags"] for row in all_db_items]
        vectors = embed_texts(tag_texts)
        
        # ë²¡í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë°ì´íŠ¸
        for i, row in enumerate(all_db_items):
            tour_spot = db.query(models.TourSpot).filter_by(contentid=row["contentid"]).first()
            if tour_spot:
                tour_spot.pref_vector = vectors[i]
        
        db.commit()
        print(f"âœ… ì„ë² ë”© ë²¡í„° ìƒì„± ì™„ë£Œ: {len(vectors)}ê°œ")
    else:
        print("\nğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. ìƒˆë¡œ ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    db.close()
    
    print(f"\n{'='*60}")
    print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ“Š ì´ ì‹ ê·œ ìˆ˜ì§‘: {total_new_items}ê±´")
    print("ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    for content_type_id, filename in CONTENT_TYPE_FILES.items():
        if content_type_id in [ct[0] for ct in content_types]:
            file_path = Path("data") / filename
            if file_path.exists():
                print(f"   - {filename}")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()