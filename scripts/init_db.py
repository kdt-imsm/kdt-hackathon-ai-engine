"""
scripts/init_db.py
==================

Database ì´ˆê¸°í™” ë° ë”ë¯¸ ë°ì´í„° ë¡œë”© ìŠ¤í¬ë¦½íŠ¸
------------------------------------------------
KDT í•´ì»¤í†¤ í”„ë¡œì íŠ¸ì˜ ë¡œì»¬ ê°œë°œÂ·ì‹œì—° í™˜ê²½ìš© PostgreSQL + pgvector ë°ì´í„°ë² ì´ìŠ¤ë¥¼
ì™„ì „íˆ ì¬ìƒì„±í•˜ê³ , CSV ê¸°ë°˜ ë”ë¯¸ ë°ì´í„°ë¥¼ UPSERT(ì‚½ì…/ê°±ì‹ )í•˜ë©°
OpenAI Embedding APIë¥¼ í†µí•´ ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

ì‹¤í–‰ ë‹¨ê³„
~~~~~~~~~
0. pgvector `vector` EXTENSION ì„¤ì¹˜
1. ê¸°ì¡´ ëª¨ë“  í…Œì´ë¸” **DROP**
2. `Base.metadata.create_all()` ë¡œ í…Œì´ë¸” **ì¬ìƒì„±**
3. `data/dummy_jobs.csv` â†’ `JobPost` í…Œì´ë¸” UPSERT
4. `JobPost` íƒœê·¸ ì„ë² ë”© ê°±ì‹ 
5. `TourSpot` ê´€ê´‘ì§€ ê°œìš” ì„ë² ë”© ê°±ì‹ 
6. `data/dummy_prefer.csv` â†’ ë”ë¯¸ ì‚¬ìš©ì ì„ í˜¸ íƒœê·¸ ë¡œë“œ

ì£¼ìš” í•¨ìˆ˜
~~~~~~~~~
- ``upsert_from_df(df, model, db)``
  Pandas DataFrameì„ ORM í…Œì´ë¸”ì— UPSERTí•©ë‹ˆë‹¤.

- ``refresh_embeddings(db, rows, attr_name="tags")``
  `embedding` í•„ë“œê°€ ë¹„ì–´ ìˆëŠ” ë ˆì½”ë“œë¥¼ ì„ë² ë”© í›„ ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ
~~~~~~~~~
```bash
python -m scripts.init_db
```

í™˜ê²½ ì˜ì¡´ì„±
~~~~~~~~~~~
- .env : DB URL, OpenAI API KEY ë“± í™˜ê²½ ë³€ìˆ˜
- app.embeddings.embedding_service.embed_texts : OpenAI Embedding í˜¸ì¶œ ë˜í¼
"""

from pathlib import Path
import pandas as pd
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.orm import Session
from sqlalchemy import text

from app.db.database import SessionLocal, Base, engine
from app.db import models
from app.embeddings.embedding_service import embed_texts

# --------------------------------------------------------------------------- #
# ìƒìˆ˜: ë”ë¯¸ CSV íŒŒì¼ ê²½ë¡œ (ì¼ê±°ë¦¬ ë°ì´í„°)
# --------------------------------------------------------------------------- #
CSV_JOBS = Path("data/dummy_jobs.csv")
CSV_TOURS = Path("data/tour_api_attractions.csv")  # ê´€ê´‘ì§€ ë°ì´í„° ì‚¬ìš©


def upsert_from_df(df: pd.DataFrame, model, db: Session):
    """DataFrameì„ ORM í…Œì´ë¸”ì— UPSERT.

    Args:
        df (pd.DataFrame): ì‚½ì…/ê°±ì‹ í•  ë°ì´í„°í”„ë ˆì„ (PK ì»¬ëŸ¼ í¬í•¨).
        model (DeclarativeMeta): SQLAlchemy ORM ëª¨ë¸ í´ë˜ìŠ¤.
        db (Session): SQLAlchemy ì„¸ì…˜ ì¸ìŠ¤í„´ìŠ¤.

    Notes:
        - `sqlalchemy.dialects.postgresql.insert` ì‚¬ìš©.
        - Primary Key ì¶©ëŒ ì‹œ `ON CONFLICT DO UPDATE`ë¡œ id ì™¸ ì»¬ëŸ¼ì„ ê°±ì‹ í•©ë‹ˆë‹¤.
    """
    # DataFrame â†’ list[dict] ë¡œ ë³€í™˜í•˜ì—¬ bulk insert í˜•íƒœë¡œ ì‘ì„±
    records = df.to_dict("records")

    insert_stmt = insert(model).values(records)

    # id(PK)ë¥¼ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼ì„ ì—…ë°ì´íŠ¸ ëŒ€ìƒìœ¼ë¡œ ì§€ì •
    update_cols = {
        c: getattr(insert_stmt.excluded, c)
        for c in df.columns if c != "id"
    }

    stmt = insert_stmt.on_conflict_do_update(
        index_elements=[model.__table__.primary_key.columns.keys()[0]],
        set_=update_cols,
    )

    db.execute(stmt)
    db.commit()


def refresh_embeddings(db: Session, rows, attr_name: str = "tags"):
    """pref_vectorê°€ ë¹„ì–´ ìˆëŠ” ë ˆì½”ë“œë§Œ ì„ë² ë”© í›„ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        db (Session): SQLAlchemy ì„¸ì…˜.
        rows (list[Any]): ORM ê°ì²´ ë¦¬ìŠ¤íŠ¸ (JobPost, TourSpot ë“±).
        attr_name (str): ì„ë² ë”© ëŒ€ìƒ í…ìŠ¤íŠ¸ê°€ ì €ì¥ëœ ì†ì„±ëª….
    """
    # 1) pref_vectorê°€ None ì¸ ë ˆì½”ë“œë§Œ í•„í„°ë§
    needs = [r for r in rows if getattr(r, "pref_vector", None) is None]
    if not needs:
        return  # ëª¨ë‘ ì„ë² ë”© ì¡´ì¬

    # 2) í…ìŠ¤íŠ¸ ì¶”ì¶œ (list â†’ ê³µë°± join, str â†’ ê·¸ëŒ€ë¡œ)
    texts = []
    for r in needs:
        val = getattr(r, attr_name)
        texts.append(" ".join(val) if isinstance(val, list) else str(val))

    # 3) OpenAI Embedding API í˜¸ì¶œ â†’ ë²¡í„° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    embeds = embed_texts(texts)

    # 4) ORM ê°ì²´ pref_vector í•„ë“œì— ë²¡í„° ì €ì¥
    for obj, vec in zip(needs, embeds):
        obj.pref_vector = vec
    db.commit()


if __name__ == "__main__":
    # --------------------------- 0) pgvector í™•ì¥ --------------------------- #
    print("â–¶ pgvector extension ì„¤ì¹˜ ì¤‘...")
    with engine.begin() as conn:  # ìë™ ì»¤ë°‹ ì»¨í…ìŠ¤íŠ¸
        conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector;"))

    # --------------------------- 1) í…Œì´ë¸” DROP ----------------------------- #
    print("â–¶ ê¸°ì¡´ í…Œì´ë¸”(drop) ì¤‘...")
    Base.metadata.drop_all(bind=engine)

    # --------------------------- 2) í…Œì´ë¸” CREATE --------------------------- #
    print("â–¶ í…Œì´ë¸”(create) ì¬ìƒì„± ì¤‘...")
    Base.metadata.create_all(bind=engine)

    # SQLAlchemy ì„¸ì…˜ ìƒì„±
    db = SessionLocal()

    # ------------------- 3) dummy_jobs.csv UPSERT --------------------------- #
    print("â–¶ dummy_jobs.csv ë¡œ ì¼ê±°ë¦¬ UPSERT ì¤‘...")
    jobs_df = pd.read_csv(CSV_JOBS)
    
    # CSV íŒŒì¼ì€ ì´ë¯¸ ì˜¬ë°”ë¥¸ ì˜ë¬¸ ì»¬ëŸ¼ëª…ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ë§¤í•‘ ë¶ˆí•„ìš”
    # í•„ìš”í•œ ì»¬ëŸ¼ë“¤ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    required_columns = ['title', 'work_date', 'work_hours', 'required_people', 'region', 'address', 'crop_type', 'preference_condition', 'image_url']
    missing_columns = [col for col in required_columns if col not in jobs_df.columns]
    if missing_columns:
        print(f"âŒ CSV íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
        print(f"ğŸ“‹ CSV ì‹¤ì œ ì»¬ëŸ¼: {list(jobs_df.columns)}")
        raise ValueError(f"CSV íŒŒì¼ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {missing_columns}")
    
    print(f"âœ… CSV íŒŒì¼ êµ¬ì¡° í™•ì¸ ì™„ë£Œ: {len(jobs_df)} ë ˆì½”ë“œ, {len(jobs_df.columns)} ì»¬ëŸ¼")
    
    # ê·¼ë¬´ì‹œê°„ì—ì„œ start_time, end_time ì¶”ì¶œ
    def parse_work_hours(work_hours):
        if pd.isna(work_hours) or work_hours == '':
            return None, None
        try:
            if '-' in str(work_hours):
                start, end = str(work_hours).split('-', 1)
                return start.strip(), end.strip()
        except:
            pass
        return None, None
    
    if 'work_hours' in jobs_df.columns:
        jobs_df[['start_time', 'end_time']] = jobs_df['work_hours'].apply(
            lambda x: pd.Series(parse_work_hours(x))
        )
    
    # tags í•„ë“œ ìƒì„± (crop_type + preference_condition ì¡°í•©)
    if 'crop_type' in jobs_df.columns and 'preference_condition' in jobs_df.columns:
        jobs_df['tags'] = jobs_df.apply(
            lambda row: f"{row.get('crop_type', '')} {row.get('preference_condition', '')}".strip(),
            axis=1
        )
    elif 'tags' not in jobs_df.columns:
        jobs_df['tags'] = 'ë†ì—… ì²´í—˜'  # ê¸°ë³¸ê°’
    
    # id ì»¬ëŸ¼ ì²˜ë¦¬
    if 'job_id' in jobs_df.columns:
        jobs_df = jobs_df.rename(columns={'job_id': 'id'})
    elif 'id' not in jobs_df.columns:
        jobs_df = jobs_df.reset_index()
        jobs_df = jobs_df.rename(columns={'index': 'id'})
        jobs_df['id'] = jobs_df['id'] + 1
    
    # ì¢Œí‘œ ì •ë³´ ìƒì„± (ì§€ì—­ ê¸°ë°˜)
    from app.utils.location import get_coordinates_from_region
    
    if 'lat' not in jobs_df.columns or 'lon' not in jobs_df.columns:
        print("   ğŸŒ ì§€ì—­ ê¸°ë°˜ ì¢Œí‘œ ìƒì„± ì¤‘...")
        coordinates = []
        
        for _, row in jobs_df.iterrows():
            region = row.get('region', '')
            lat, lon = get_coordinates_from_region(region)
            coordinates.append({'lat': lat, 'lon': lon})
        
        coords_df = pd.DataFrame(coordinates)
        jobs_df['lat'] = coords_df['lat']
        jobs_df['lon'] = coords_df['lon']
        
        print(f"   âœ… ì¢Œí‘œ ìƒì„± ì™„ë£Œ: ìœ íš¨í•œ ì¢Œí‘œ {sum(jobs_df['lat'].notna())}ê°œ")
    else:
        print("   âœ… ê¸°ì¡´ ì¢Œí‘œ ì •ë³´ ì‚¬ìš©")
    
    upsert_from_df(jobs_df, models.JobPost, db)

    # ------------------- 3-2) ê´€ê´‘ì§€ CSV í†µí•© ë¡œë“œ --------------------------- #
    print("â–¶ ê´€ê´‘ì§€ ë°ì´í„° í†µí•© ë¡œë“œ ì¤‘...")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ê´€ê´‘ì§€ CSV íŒŒì¼ë“¤
    tour_files = [
        "data/tour_api_attractions.csv",
        "data/tour_api_cultural.csv", 
        "data/tour_api_festivals.csv",
        "data/tour_api_courses.csv",
        "data/tour_api_leisure.csv",
        "data/tour_api_shopping.csv"
    ]
    
    all_tours_df = pd.DataFrame()
    
    for file_path in tour_files:
        if Path(file_path).exists():
            try:
                df = pd.read_csv(file_path)
                print(f"   âœ… {Path(file_path).name}: {len(df)}ê±´ ë¡œë“œ")
                all_tours_df = pd.concat([all_tours_df, df], ignore_index=True)
            except Exception as e:
                print(f"   âš ï¸ {Path(file_path).name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print(f"   âŒ {Path(file_path).name} íŒŒì¼ ì—†ìŒ")
    
    if len(all_tours_df) > 0:
        # ë°ì´í„° ì •ì œ: TourSpot ëª¨ë¸ê³¼ ì¼ì¹˜ì‹œí‚¤ê¸°
        print("   ğŸ”§ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ë¦¬ ì¤‘...")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ (TourSpot ëª¨ë¸ì— ë§ê²Œ)
        required_columns = ['name', 'region', 'tags', 'lat', 'lon', 'contentid', 'image_url', 'detailed_keywords', 'keywords']
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ìœ ì§€
        available_columns = [col for col in required_columns if col in all_tours_df.columns]
        clean_tours_df = all_tours_df[available_columns].copy()
        
        # ëˆ„ë½ëœ ì»¬ëŸ¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        for col in required_columns:
            if col not in clean_tours_df.columns:
                if col in ['detailed_keywords']:
                    clean_tours_df[col] = "[]"
                elif col in ['keywords', 'image_url']:
                    clean_tours_df[col] = ""
                else:
                    clean_tours_df[col] = None
        
        # id ì»¬ëŸ¼ ì¶”ê°€ (ì¸ë±ìŠ¤ ê¸°ë°˜)
        clean_tours_df = clean_tours_df.reset_index()
        clean_tours_df = clean_tours_df.rename(columns={'index': 'id'})
        clean_tours_df['id'] = clean_tours_df['id'] + 1
        
        # ì¤‘ë³µ í–‰ ì œê±° (contentid ê¸°ì¤€)
        if 'contentid' in clean_tours_df.columns:
            clean_tours_df = clean_tours_df.drop_duplicates(subset=['contentid'], keep='first')
        
        print(f"   ğŸ“Š ì •ì œëœ ê´€ê´‘ì§€ ë°ì´í„°: {len(clean_tours_df)}ê±´")
        print(f"   ğŸ“‹ ì‚¬ìš© ì»¬ëŸ¼: {list(clean_tours_df.columns)}")
        
        upsert_from_df(clean_tours_df, models.TourSpot, db)
    else:
        print("   âŒ ë¡œë“œí•  ê´€ê´‘ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # --------------------- 4) JobPost ì„ë² ë”© ê°±ì‹  --------------------------- #
    print("â–¶ ì¼ê±°ë¦¬ embedding ê°±ì‹  ì¤‘...")
    job_rows = db.query(models.JobPost).all()
    refresh_embeddings(db, job_rows, attr_name="tags")

    # --------------------- 5) TourSpot ì„ë² ë”© ê°±ì‹  -------------------------- #
    print("â–¶ ê´€ê´‘ì§€ embedding ê°±ì‹  ì¤‘...")
    tour_rows = db.query(models.TourSpot).all()
    
    # ë²¡í„°ê°€ ì—†ëŠ” ê´€ê´‘ì§€ë§Œ í•„í„°ë§
    tours_needing_embedding = []
    texts_for_embedding = []
    
    for tour in tour_rows:
        if tour.pref_vector is None:
            tours_needing_embedding.append(tour)
            
            # keywords ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í™œìš©, ì—†ìœ¼ë©´ name + tags ì‚¬ìš©
            if hasattr(tour, 'keywords') and tour.keywords and tour.keywords.strip():
                text_for_embedding = f"{tour.name} {tour.keywords} {tour.tags}"
            else:
                text_for_embedding = f"{tour.name} {tour.tags}"
            
            texts_for_embedding.append(text_for_embedding)
    
    print(f"   ğŸ’¡ ì„ë² ë”©ì´ í•„ìš”í•œ ê´€ê´‘ì§€: {len(tours_needing_embedding)}ê°œ")
    
    # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
    if tours_needing_embedding:
        embeddings = embed_texts(texts_for_embedding)
        
        # ORM ê°ì²´ì— ë²¡í„° í• ë‹¹
        for tour, embedding in zip(tours_needing_embedding, embeddings):
            tour.pref_vector = embedding
    
    db.commit()
    print("âœ… ê´€ê´‘ì§€ ì„ë² ë”© ê°±ì‹  ì™„ë£Œ (í‚¤ì›Œë“œ í¬í•¨)")

    # ----------------- 6) ìˆ™ë°•Â·ìŒì‹ì  ë°ì´í„° ë¡œë”© --------------------------- #
    print("â–¶ ìˆ™ë°•Â·ìŒì‹ì  ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # ìˆ™ë°• ë°ì´í„°
    accommodations_file = Path("data/accommodations.csv")
    if accommodations_file.exists():
        try:
            acc_df = pd.read_csv(accommodations_file)
            acc_df = acc_df.reset_index()
            acc_df = acc_df.rename(columns={'index': 'id'})
            acc_df['id'] = acc_df['id'] + 1
            upsert_from_df(acc_df, models.Accommodation, db)
            print(f"   âœ… ìˆ™ë°•ì‹œì„¤: {len(acc_df)}ê±´ ë¡œë“œ")
        except Exception as e:
            print(f"   âš ï¸ ìˆ™ë°•ì‹œì„¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        print("   âŒ accommodations.csv íŒŒì¼ ì—†ìŒ")
    
    # ìŒì‹ì  ë°ì´í„°
    restaurants_file = Path("data/restaurants.csv")
    if restaurants_file.exists():
        try:
            rest_df = pd.read_csv(restaurants_file)
            rest_df = rest_df.reset_index()
            rest_df = rest_df.rename(columns={'index': 'id'})
            rest_df['id'] = rest_df['id'] + 1
            upsert_from_df(rest_df, models.Restaurant, db)
            print(f"   âœ… ìŒì‹ì : {len(rest_df)}ê±´ ë¡œë“œ")
        except Exception as e:
            print(f"   âš ï¸ ìŒì‹ì  ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        print("   âŒ restaurants.csv íŒŒì¼ ì—†ìŒ")
    
    # ìˆ™ë°•Â·ìŒì‹ì  ì„ë² ë”© ìƒì„±
    print("â–¶ ìˆ™ë°•Â·ìŒì‹ì  ì„ë² ë”© ìƒì„± ì¤‘...")
    
    # ìˆ™ë°•ì‹œì„¤ ì„ë² ë”©
    acc_rows = db.query(models.Accommodation).all()
    refresh_embeddings(db, acc_rows, attr_name="tags")
    
    # ìŒì‹ì  ì„ë² ë”©
    rest_rows = db.query(models.Restaurant).all()
    refresh_embeddings(db, rest_rows, attr_name="tags")
    
    print("âœ… ìˆ™ë°•Â·ìŒì‹ì  ì„ë² ë”© ì™„ë£Œ")

    # ----------------- 7) ë”ë¯¸ ì‚¬ìš©ì ì„ í˜¸ íƒœê·¸ ë¡œë”© ------------------------ #
    print("â–¶ ë”ë¯¸ ì‚¬ìš©ì ì„ í˜¸ íƒœê·¸ ë¡œë”© ì¤‘...")
    dummy_prefer_file = Path("data/dummy_prefer.csv")
    if dummy_prefer_file.exists():
        try:
            # ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—° import
            from app.db.crud import load_dummy_preferences
            load_dummy_preferences(db, "data/dummy_prefer.csv")
            print("âœ… ë”ë¯¸ ì„ í˜¸ íƒœê·¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë”ë¯¸ ì„ í˜¸ íƒœê·¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    else:
        print("âŒ dummy_prefer.csv íŒŒì¼ ì—†ìŒ (ê±´ë„ˆë›°ê¸°)")

    # ì„¸ì…˜ ì¢…ë£Œ
    db.close()
    print("âœ… DB ì´ˆê¸°í™” ì™„ë£Œ!")
